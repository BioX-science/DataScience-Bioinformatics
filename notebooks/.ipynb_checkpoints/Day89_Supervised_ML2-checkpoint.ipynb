{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning on mice phenotype data\n",
    "\n",
    "Predicting diet from differential expression data was easy with SVMs. It was very neat and regular data, no cells were missing, all values were in a similar range, etc. We will now use a slightly uglier dataset: the phenotype tables from days 3/4.\n",
    "\n",
    "You may remember that each of those sheets had one row per strain, and two separate columns for each measurement taken under the two dietary conditions. We have transformed those sheets such that 1) all of them are contained in a single table, 2) each strain gets two rows, one for phenotype measurements under CD and one for HFD diet. We will use the `diet` column as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.model_selection as sm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics as sme\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 181)\n"
     ]
    }
   ],
   "source": [
    "pheno = pd.read_csv('files/phenotype_cd_hfd.csv', index_col=0)\n",
    "\n",
    "print(pheno.shape)\n",
    "\n",
    "target = pheno['diet'].replace('CD', 0).replace('HFD', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get rid of columns with missing values\n",
    "\n",
    "Since most ML algorithms can't deal with NaN values, we will first restrict ourselves to those features that are available for every sample.\n",
    "Identify these columns and put `pheno.loc[:, good_columns]` into the variable `data`.\n",
    "\n",
    "Also, drop the columns `diet` and `strain` from the data table, since we don't want to use them for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pheno.dropna(axis=1)\n",
    "data = data.drop(['strain', 'diet'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Use an SVM for your predictions\n",
    "Try the RBF kernel for a change. First, fit and score using the entire dataset, and print out the accuracy.\n",
    "Do a proper evaluation using 3-fold cross-validation, and print those scores as well. How did it go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The RBF kernel is the default\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(data, target)\n",
    "\n",
    "clf.predict(data)\n",
    "clf.score(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4954954954954955"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.cross_val_score(clf, data, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.4954954954954955 for the 3-fold accuracy. This is even slightly worse than a random assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use a linear kernel to get the same two values\n",
    "Was it better or worse than with the RBF? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823008849557522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_linear = SVC(kernel ='linear', random_state=0, probability=True)\n",
    "clf_linear.fit(data, target)\n",
    "\n",
    "clf_linear.predict(data)\n",
    "clf_linear.score(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8589378852536748"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.cross_val_score(clf_linear, data, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel performs way better than the rbf kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Standardize the data\n",
    "Look at the value ranges of each feature. Standardize them, such that they all have zero mean and 1 standard deviation. Either by simply subtracting the means and dividing by the standard deviations, or using the `sklearn.preprocessing.StandardScaler` class.\n",
    "\n",
    "Display the cross-validated scores using an RBF and a linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "\n",
    "data_standardized = scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9023233760075865"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rbf kernel\n",
    "clf.fit(data_standardized, target)\n",
    "\n",
    "sm.cross_val_score(clf, data_standardized, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9027975343764818"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#linear kernel\n",
    "clf_linear.fit(data_standardized, target)\n",
    "\n",
    "sm.cross_val_score(clf_linear, data_standardized, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features with a high range in variance bias the svm into looking just for those certain feature, finding patterns there and not paying that much attention to the remaining. \n",
    "By standartizing the dataset, all features were roughly equal important, and the SVM is able to perform a better seperation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Optional: Standardize the data fold-by-fold\n",
    "\n",
    "When we standardized the entire dataset in one go, we were cheating a bit. We did not keep the training and test data fully independent. For a truly honest evaluation, we should derive the standardization parameters from the training data only, and apply the same transformation to the test data separately.\n",
    "\n",
    "If you standardize manually, use the training set means and std's for the transformation of both the training and the test data. If you use `StandardScaler`, use `fit_transform` for the training data and `transform` only for the training data.\n",
    "\n",
    "Did it influence the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.8401559454191032\n",
      "Specificity: 0.9122807017543858\n",
      "Precision: 0.9069272445820434\n"
     ]
    }
   ],
   "source": [
    "# Better safe than sorry: I used the k-fold method from the last sheet to check the pandas function for correctness. \n",
    "# You find the pandas-like solution in the next cell and you will be more than happy to hear that it works quite well!\n",
    "\n",
    "cv = sm.StratifiedKFold(3, shuffle=True)\n",
    "\n",
    "sensitivity = 0\n",
    "specificity = 0\n",
    "precision = 0\n",
    "\n",
    "for training, test in cv.split(data, target):\n",
    "    \n",
    "    data_train = data.iloc[training]\n",
    "    data_train_stand = scaler.fit_transform(data_train)\n",
    "    target_train = target.iloc[training]\n",
    "    \n",
    "    \n",
    "    data_test = data.iloc[test]\n",
    "    data_test_stand = scaler.transform(data_test)\n",
    "\n",
    "    target_test = target.iloc[test]\n",
    "    \n",
    "    clf_linear.fit(data_train_stand, target_train)\n",
    "    \n",
    "    y_pred = clf_linear.predict(data_test_stand)\n",
    "    y_true = target_test\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    tp = conf_matrix[1][1]\n",
    "    fp = conf_matrix[0][1]\n",
    "    tn = conf_matrix[0][0]\n",
    "    fn = conf_matrix[1][0]\n",
    "\n",
    "    sensitivity += tp/(tp+fn)\n",
    "    specificity += tn/(tn+fp)\n",
    "    precision += tp/(tp + fp)\n",
    " \n",
    "print('Sensitivity: ' + str(sensitivity/3))\n",
    "print('Specificity: ' + str(specificity/3))\n",
    "print('Precision: ' + str(precision/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             precision    recall  f1-score   support\\n\\n          0       0.89      0.89      0.89        57\\n          1       0.89      0.89      0.89        56\\n\\navg / total       0.89      0.89      0.89       113\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OR\n",
    "predictions = sm.cross_val_predict(clf_linear, data, target, cv = sm.StratifiedKFold(3, shuffle=True))\n",
    "\n",
    "sme.classification_report(target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fold-by-fold standardization make the results worse again. This is actually more legit, by standartizing the whole dataset we already take the data which is later used for testing in account. In a \"real\" case the SVM can't know about the data it will face, so the fold-by-fold method is the way to go if you don't want to cheat on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Sensitivity, specificity, precision...\n",
    "In some cases, the accuracy of a prediction is secondary to other quality measures, such as sensitivity or specificity. For example, HIV tests are optimized for sensitivity at the expense of accuracy, ensuring that very few HIV-positive individuals test negative on an HIV test. This results in an HIV-scare for a lot of HIV-negative individuals each year (as higher sensitivity always implies a higher false positive rate) but in exchange no case of HIV goes undetected on a test.\n",
    "\n",
    "We can tune most ML models similarly, and sacrifice accuracy for higher sensitivity or specificity. But first, simply report the sensitivity of your linear SVM for both classes. You will find tools in `sklearn` that help you calculate this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is already implemented in the cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Make your SVM 95+% sensitive for HFD\n",
    "Find a parameter that helps you increase your sensitivity for mice on an HFD diet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        57\n",
      "          1       0.50      1.00      0.66        56\n",
      "\n",
      "avg / total       0.25      0.50      0.33       113\n",
      "\n",
      "Sensitivity: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Make use of the class_weight parameter. HFD gets 100% class weight -> every point is assigned to it -> no point is missed \n",
    "# -> every point is captured  = 100% sensitive for HFD\n",
    "\n",
    "clf_sensitive = SVC(random_state=0, kernel='linear', class_weight={0: 0.00, 1: 1.00})\n",
    "\n",
    "clf_sensitive.fit(data, target)\n",
    "\n",
    "prediction = cross_val_predict(clf_sensitive, data, target, cv = cv)\n",
    "\n",
    "print(sme.classification_report(target, prediction))\n",
    "\n",
    "conf_matrix = confusion_matrix(target, prediction)\n",
    "    \n",
    "tp = conf_matrix[1][1]\n",
    "fp = conf_matrix[0][1]\n",
    "tn = conf_matrix[0][0]\n",
    "fn = conf_matrix[1][0]\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "specificity = tn/(tn+fp)\n",
    "precision = tp/(tp + fp)\n",
    "\n",
    "print('Sensitivity: '+ str(sensitivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 ROC curves\n",
    "You might be interested in your model's relationship between its accuracy and sensitivity, or a more commonly used pair of quality measures: false positive rate vs. sensitivity (aka true positive rate). This is what ROC (receiver operating characteristic) curves display: the trade-off between these two qualities.\n",
    "\n",
    "Most classification ML methods, despite their categorical output, use continuous internal variables for their predictions, and their final decision is a simple thresholding of this continuous variable. For example, in the case of SVMs, this variable is the data point's signed distance to the separating plane: positive values are assigned to one class, negative values to the other class. Values close to zero (= close to the boundary) are harder to place in either class, and it's down to the arbitrary threshold how they end up being predicted.\n",
    "\n",
    "You can create a ROC curve by testing how the choice of threshold affects false positive rate and sensitivity. Needless to say, sklearn helps you create such plots. All you need to do is extract the SVM's continuous predictive variables, pass it to the appropriate function with the true labels, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sm.cross_val_predict(clf_linear, data, target, cv = sm.StratifiedKFold(3, shuffle=True), method='predict_proba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for i in predictions:\n",
    "    values.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = sme.roc_curve(target, values)\n",
    "\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.title('ROC curve')\n",
    "roc_curve = plt.plot(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Find the threshold for the desired sensitivity / FPR tradeoff\n",
    "In 1.5.2 you increased sensitivity for HFD (label 1) by telling the SVM to use a higher weight for that class. Since then, you have learned that you could have also used the SVM's continuous predictive variables, and threshold them to your own liking, instead of leaving it to the SVM's default (0 for decision_function and 0.5 for predict_proba).\n",
    "\n",
    "Your task is to find the threshold value that would suit your purpose (i.e. 95% sensitivity). Remember, the roc_curve function returned three vectors: the ROC plot's FPR values, sensitivity values and the threshold that corresponded to them.\n",
    "\n",
    "Hint: iterate over the sensitivity and threshold values together, and report the first threshold where sensitivity exceeds 0.95. You can iterate over two lists together using Python's zip function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14102238823299776\n"
     ]
    }
   ],
   "source": [
    "for sensitivity, threshold in zip(tpr, thresholds):\n",
    "    if sensitivity > 0.95:\n",
    "        print (threshold)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
