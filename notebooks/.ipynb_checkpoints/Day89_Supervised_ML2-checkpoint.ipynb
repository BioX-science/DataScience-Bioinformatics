{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning on mice phenotype data\n",
    "\n",
    "Predicting diet from differential expression data was easy with SVMs. It was very neat and regular data, no cells were missing, all values were in a similar range, etc. We will now use a slightly uglier dataset: the phenotype tables from days 3/4.\n",
    "\n",
    "You may remember that each of those sheets had one row per strain, and two separate columns for each measurement taken under the two dietary conditions. We have transformed those sheets such that 1) all of them are contained in a single table, 2) each strain gets two rows, one for phenotype measurements under CD and one for HFD diet. We will use the `diet` column as our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.model_selection as sm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics as sme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 181)\n"
     ]
    }
   ],
   "source": [
    "pheno = pd.read_csv('files/phenotype_cd_hfd.csv', index_col=0)\n",
    "\n",
    "print(pheno.shape)\n",
    "\n",
    "target = pheno['diet'].replace('CD', 0).replace('HFD', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get rid of columns with missing values\n",
    "\n",
    "Since most ML algorithms can't deal with NaN values, we will first restrict ourselves to those features that are available for every sample.\n",
    "Identify these columns and put `pheno.loc[:, good_columns]` into the variable `data`.\n",
    "\n",
    "Also, drop the columns `diet` and `strain` from the data table, since we don't want to use them for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pheno.dropna(axis=1)\n",
    "data = data.drop(['strain', 'diet'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Use an SVM for your predictions\n",
    "Try the RBF kernel for a change. First, fit and score using the entire dataset, and print out the accuracy.\n",
    "Do a proper evaluation using 3-fold cross-validation, and print those scores as well. How did it go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4954954954954955"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(random_state=0)\n",
    "clf.fit(data, target)\n",
    "\n",
    "sm.cross_val_score(clf, data, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use a linear kernel to get the same two values\n",
    "Was it better or worse than with the RBF? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8674727358937885"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_linear = SVC(kernel ='linear', random_state=0, class_weight = {1: 100}, probability=True)\n",
    "clf_linear.fit(data, target)\n",
    "\n",
    "sm.cross_val_score(clf_linear, data, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Standardize the data\n",
    "Look at the value ranges of each feature. Standardize them, such that they all have zero mean and 1 standard deviation. Either by simply subtracting the means and dividing by the standard deviations, or using the `sklearn.preprocessing.StandardScaler` class.\n",
    "\n",
    "Display the cross-validated scores using an RBF and a linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.60930619e+01 3.80416726e+01 4.27160841e+01 4.20416752e+01\n",
      " 3.85055428e+01 4.20477504e+01 3.92453392e+01 1.20487950e+01\n",
      " 4.58936389e+01 1.66053788e+01 6.39649935e+01 6.31652998e+00\n",
      " 7.99026793e+01 1.08702642e+02 1.24067920e+02 3.12049115e+02\n",
      " 3.38286991e+02 3.36154159e+02 3.20834558e+02 2.15199027e+02\n",
      " 1.83235310e+02 4.53513279e+04 2.01888346e+02 8.84909119e+00\n",
      " 8.61486272e+00 3.72535251e+01 3.60611652e+01 3.56086136e+01\n",
      " 3.53139233e+01 3.51120649e+01 3.49989676e+01 3.48695133e+01\n",
      " 5.15699754e+00 6.22915051e+00 1.29537584e+00 3.35208862e+03\n",
      " 2.20468813e+03 4.49369912e+03 4.22855634e+01 3.09761463e+02\n",
      " 5.48634823e+02 2.39153596e+02 1.61580427e-01 2.98506596e+00\n",
      " 3.15319164e+00 1.80977319e+00 1.26817723e+00 5.08332696e-01\n",
      " 1.78550112e+00 3.03983431e-01 4.21409437e+00 2.75902843e+00\n",
      " 1.25669752e+00 4.07605629e+00 7.51302251e-01]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "#StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(scaler.mean_)\n",
    "data_standardized = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.929113323850166"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_standardized, target)\n",
    "\n",
    "sm.cross_val_score(clf, data_standardized, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9027975343764818"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_linear.fit(data_standardized, target)\n",
    "\n",
    "sm.cross_val_score(clf_linear, data_standardized, target, cv = sm.StratifiedKFold(3, shuffle=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features with high range \"bias\" the svm in looking just for this certain column, finding patterns there and did not pay that much attention to the others . \n",
    "By standartize the dataset, all features were roughly equal important, and the SVM was abel to perform a better seperation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Optional: Standardize the data fold-by-fold\n",
    "\n",
    "When we standardized the entire dataset in one go, we were cheating a bit. We did not keep the training and test data fully independent. For a truly honest evaluation, we should derive the standardization parameters from the training data only, and apply the same transformation to the test data separately.\n",
    "\n",
    "If you standardize manually, use the training set means and std's for the transformation of both the training and the test data. If you use `StandardScaler`, use `fit_transform` for the training data and `transform` only for the training data.\n",
    "\n",
    "Did it influence the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8947368421052632 0.8421052631578947 0.85\n",
      "0.9473684210526315 0.9473684210526315 0.9473684210526315\n",
      "0.8333333333333334 0.9473684210526315 0.9375\n"
     ]
    }
   ],
   "source": [
    "cv = sm.StratifiedKFold(3, shuffle=True)\n",
    "\n",
    "for training, test in cv.split(data, target):\n",
    "    \n",
    "    data_train = data.iloc[training]\n",
    "    data_train_stand = scaler.fit_transform(data_train)\n",
    "    target_train = target.iloc[training]\n",
    "    \n",
    "    \n",
    "    data_test = data.iloc[test]\n",
    "    data_test_stand = scaler.transform(data_test)\n",
    "\n",
    "    target_test = target.iloc[test]\n",
    "    \n",
    "    clf_linear.fit(data_train_stand, target_train)\n",
    "    #print(clf_linear.score(data_test, target_test))\n",
    "    \n",
    "    y_pred = clf_linear.predict(data_test_stand)\n",
    "    y_true = target_test\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    conf_matrix\n",
    "    \n",
    "    tp = conf_matrix[1][1]\n",
    "    fp = conf_matrix[0][1]\n",
    "    tn = conf_matrix[0][0]\n",
    "    fn = conf_matrix[1][0]\n",
    "\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    precision = tp/(tp + fp)\n",
    "\n",
    "    print(sensitivity, specificity, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             precision    recall  f1-score   support\\n\\n          0       0.79      0.84      0.81        57\\n          1       0.83      0.77      0.80        56\\n\\navg / total       0.81      0.81      0.81       113\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OR\n",
    "predictions = sm.cross_val_predict(clf_linear, data, target, cv = sm.StratifiedKFold(3, shuffle=True))\n",
    "\n",
    "sme.classification_report(target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Sensitivity, specificity, precision...\n",
    "In some cases, the accuracy of a prediction is secondary to other quality measures, such as sensitivity or specificity. For example, HIV tests are optimized for sensitivity at the expense of accuracy, ensuring that very few HIV-positive individuals test negative on an HIV test. This results in an HIV-scare for a lot of HIV-negative individuals each year (as higher sensitivity always implies a higher false positive rate) but in exchange no case of HIV goes undetected on a test.\n",
    "\n",
    "We can tune most ML models similarly, and sacrifice accuracy for higher sensitivity or specificity. But first, simply report the sensitivity of your linear SVM for both classes. You will find tools in `sklearn` that help you calculate this value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Make your SVM 95+% sensitive for HFD\n",
    "Find a parameter that helps you increase your sensitivity for mice on an HFD diet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter = class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 ROC curves\n",
    "You might be interested in your model's relationship between its accuracy and sensitivity, or a more commonly used pair of quality measures: false positive rate vs. sensitivity (aka true positive rate). This is what ROC (receiver operating characteristic) curves display: the trade-off between these two qualities.\n",
    "\n",
    "Most classification ML methods, despite their categorical output, use continuous internal variables for their predictions, and their final decision is a simple thresholding of this continuous variable. For example, in the case of SVMs, this variable is the data point's signed distance to the separating plane: positive values are assigned to one class, negative values to the other class. Values close to zero (= close to the boundary) are harder to place in either class, and it's down to the arbitrary threshold how they end up being predicted.\n",
    "\n",
    "You can create a ROC curve by testing how the choice of threshold affects false positive rate and sensitivity. Needless to say, sklearn helps you create such plots. All you need to do is extract the SVM's continuous predictive variables, pass it to the appropriate function with the true labels, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sm.cross_val_predict(clf_linear, data, target, cv = sm.StratifiedKFold(3, shuffle=True), method='predict_proba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for i in range(len(predictions)):\n",
    "    values.append(predictions[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.018500439175373137,\n",
       " 0.2532034418664303,\n",
       " 0.8049570521829292,\n",
       " 0.943728271056083,\n",
       " 0.026909654448517076,\n",
       " 0.7893160426341753,\n",
       " 0.4768507503817682,\n",
       " 0.994959969986122,\n",
       " 0.04663328912501736,\n",
       " 0.22866391691717985,\n",
       " 0.12374581565533166,\n",
       " 0.996088080179374,\n",
       " 0.017550638993684677,\n",
       " 0.5257821499759743,\n",
       " 0.1361565637989083,\n",
       " 0.9447848493906761,\n",
       " 0.021865733226327965,\n",
       " 0.8629104710539175,\n",
       " 0.21466013547730084,\n",
       " 0.6756147621534483,\n",
       " 0.697149074805842,\n",
       " 0.9845367503071547,\n",
       " 0.03615009312936576,\n",
       " 0.3502795241760625,\n",
       " 0.09149031537076695,\n",
       " 0.9816855466795898,\n",
       " 0.6899746208880974,\n",
       " 0.9999943520512725,\n",
       " 0.10643323519016863,\n",
       " 0.9863123534968082,\n",
       " 0.08256488222590924,\n",
       " 0.9929125326137873,\n",
       " 0.41414050821933823,\n",
       " 0.988214942623464,\n",
       " 0.03349660829357062,\n",
       " 0.26063827493464803,\n",
       " 0.06018318929648738,\n",
       " 0.8836709971168376,\n",
       " 0.09024264613926912,\n",
       " 0.996971553201003,\n",
       " 0.12392160025738848,\n",
       " 0.9077294160728864,\n",
       " 0.0360705549648152,\n",
       " 0.8631441353352957,\n",
       " 0.0011847267583356353,\n",
       " 0.4384508910163755,\n",
       " 0.1962729867263032,\n",
       " 0.9735644935361585,\n",
       " 0.0786662904986566,\n",
       " 0.9845362288255912,\n",
       " 0.5874231629155194,\n",
       " 0.9851786856265979,\n",
       " 0.02561137686790177,\n",
       " 0.7886554328702313,\n",
       " 0.18558389812620193,\n",
       " 0.9781075631655471,\n",
       " 0.03060890931694724,\n",
       " 0.7567677341997325,\n",
       " 0.48793715146362227,\n",
       " 0.9974559448284169,\n",
       " 0.07599505330917472,\n",
       " 0.5,\n",
       " 0.30781225895102804,\n",
       " 0.19880552403270768,\n",
       " 0.9905585387518774,\n",
       " 0.29583277875315533,\n",
       " 0.981730094326305,\n",
       " 0.1864683100397623,\n",
       " 0.9795449531776835,\n",
       " 0.094501234268694,\n",
       " 0.5528109254442498,\n",
       " 0.006426754378746493,\n",
       " 0.5938296445587615,\n",
       " 0.4626953106908814,\n",
       " 0.9289818746134649,\n",
       " 0.009892801467524052,\n",
       " 0.8274429914746686,\n",
       " 0.5717940592950902,\n",
       " 0.9426694626126655,\n",
       " 0.16305464305783376,\n",
       " 0.7628527793461126,\n",
       " 0.023984736178869124,\n",
       " 0.7337872229486935,\n",
       " 0.09230624134461636,\n",
       " 0.9242345454656801,\n",
       " 0.265681928508533,\n",
       " 0.9892869735602101,\n",
       " 0.05306752635967264,\n",
       " 0.8741038020333397,\n",
       " 0.0367106737660621,\n",
       " 0.9970831292284146,\n",
       " 0.08013733655047606,\n",
       " 0.6442288781118679,\n",
       " 0.15680941845465615,\n",
       " 0.9972117196005262,\n",
       " 0.8715689760906766,\n",
       " 0.9955886006009828,\n",
       " 0.1782092668691704,\n",
       " 0.8735180052896143,\n",
       " 0.01607396807734943,\n",
       " 0.509601334294712,\n",
       " 0.3391785294278139,\n",
       " 0.9999950676738317,\n",
       " 0.34913331194138014,\n",
       " 0.9893195880646167,\n",
       " 0.2531757037749692,\n",
       " 0.6291334172790014,\n",
       " 0.17368424144378491,\n",
       " 0.9034784793588064,\n",
       " 0.10174684900302766,\n",
       " 0.9702649652891463,\n",
       " 0.4785841202954526,\n",
       " 0.8732450724451286]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_results = sme.roc_curve(target, values)\n",
    "\n",
    "roc_curve = plt.plot(roc_results[0], roc_results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
